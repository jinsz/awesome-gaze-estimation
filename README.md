<!--div id="readme" class="readme blob instapaper_body">
<article class="markdown-body entry-content" itemprop="text"><h1><a id="user-content-awesome-work-on-gaze-estimation" class="anchor" aria-hidden="true" href="#awesome-work-on-gaze-estimation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Awesome Work on Gaze Estimation</h1-->
<h2><a id="user-content-table-of-contents" class="anchor" aria-hidden="true" href="#table-of-contents"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Table of Contents</h2>

<ul>
<!--li><a href="#evaluation">Evaluation</a></li-->
<!--li><a href="#arxiv-papers">arXiv Papers</a></li-->
<li><a href="#review-papers">Review Papers</a></li>
<li><a href="#journal-papers">Journal Papers</a></li>
<li><a href="#conference-papers">Conference Papers</a>
<ul>
<li><a href="#cvpr">CVPR</a></li>
<li><a href="#iccv">ICCV</a></li>
<li><a href="#eccv">ECCV</a></li>
<!--li><a href="#etra">ETRA</a></li-->
<li><a href="#others">Others</a></li>
</ul>
</li>
<!--li><a href="#theses">Theses</a></li-->
<li><a href="#datasets">Datasets</a></li>
<!--li><a href="#workshops">Workshops</a></li-->
<!--li><a href="#challenges">Challenges</a></li>
<li><a href="#other-related-papers">Other Related Papers</a></li-->
</ul>

<!--h2><a id="user-content-evaluation" class="anchor" aria-hidden="true" href="#evaluation"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Evaluation</h2>
<p>See folder <a href="https://github.com/xinghaochen/awesome-hand-pose-estimation/tree/master/evaluation"><code>evaluation</code></a> to get more details about performance evaluation for hand pose estimation.</p-->


<!--h2><a id="user-content-arxiv-papers" class="anchor" aria-hidden="true" href="#arxiv-papers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>arXiv Papers</h2>
<h5><a id="user-content-arxiv180906268-vision-based-teleoperation-of-shadow-dexterous-hand-using-end-to-end-deep-neural-network-pdf" class="anchor" aria-hidden="true" href="#arxiv180906268-vision-based-teleoperation-of-shadow-dexterous-hand-using-end-to-end-deep-neural-network-pdf"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><a href="https://arxiv.org/abs/1809.06268" rel="nofollow">[arXiv:1809.06268]</a> Vision-based Teleoperation of Shadow Dexterous Hand using End-to-End Deep Neural Network. <a href="https://arxiv.org/pdf/1809.06268.pdf" rel="nofollow">[PDF]</a></h5>
<p><em>Shuang Li*, Xiaojian Ma*, Hongzhuo Liang, Michael GÃ¶rner, Philipp Ruppel, Bing Fang, Fuchun Sun, Jianwei Zhang</em></p-->

<h2><a id="user-content-review-papers" class="anchor" aria-hidden="true" href="#review-papers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Review Papers</h2>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Hansen2010 TPAMI] In the Eye of the Beholder: A Survey of Models for Eyes and Gaze. <a href="https://ieeexplore.ieee.org/document/4770110" rel="nofollow">[PDF]</a>
<p><em>D.W. Hansen and Qiang Ji</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Morimoto-Mimica2005 CVIU] Eye gaze tracking techniques for interactive applications. <a href="https://www.sciencedirect.com/science/article/pii/S1077314204001109" rel="nofollow">[PDF]</a>
<p><em>Carlos H. Morimoto and Marcio R.M. Mimica</em></p>

<h2><a id="user-content-journal-papers" class="anchor" aria-hidden="true" href="#journal-papers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Journal Papers</h2>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Zhang-etal2017 TPAMI] MPIIGaze: Real-World Dataset and Deep Appearance-Based Gaze Estimation. <a href="https://ieeexplore.ieee.org/document/8122058" rel="nofollow">[PDF]</a>
<p><em>Xucong Zhang and Yusuke Sugano and Mario Fritz and Andreas Bulling</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Lu-etal2014 TPAMI] Adaptive Linear Regression for Appearance-Based Gaze Estimation. <a href="https://ieeexplore.ieee.org/document/6777326" rel="nofollow">[PDF]</a>
<p><em>Feng Lu and Yusuke Sugano and Takahiro Okabe and Yoichi Sato</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Sugano-etal2013 TPAMI] Appearance-Based Gaze Estimation Using Visual Saliency. <a href="https://ieeexplore.ieee.org/document/6193107" rel="nofollow">[PDF]</a>
<p><em>Yusuke Sugano and Yasuyuki Matsushita and Yoichi Sato</em></p>

<!--h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Lu-etal2012 ICPR] Head pose-free appearance-based gaze sensing via eye image synthesis. <a href="https://ieeexplore.ieee.org/document/6460306" rel="nofollow">[PDF]</a>
<p><em>Feng Lu and Yusuke Sugano and Takahiro Okabe and Yoichi Sato</em></p>
<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Valenti-etal2012 TIP] Combining Head Pose and Eye Location Information for Gaze Estimation. <a href="https://ieeexplore.ieee.org/document/5959981" rel="nofollow">[PDF]</a>
<p><em>R. Valenti and N. Sebe and T. Gevers</em></p-->

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Hansen2010 TPAMI] In the Eye of the Beholder: A Survey of Models for Eyes and Gaze. <a href="https://ieeexplore.ieee.org/document/4770110" rel="nofollow">[PDF]</a>
<p><em>D.W. Hansen and Qiang Ji</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Morimoto-Mimica2005 CVIU] Eye gaze tracking techniques for interactive applications. <a href="https://www.sciencedirect.com/science/article/pii/S1077314204001109" rel="nofollow">[PDF]</a>
<p><em>Carlos H. Morimoto and Marcio R.M. Mimica</em></p>

<h2><a id="user-content-conference-papers" class="anchor" aria-hidden="true" href="#conference-papers"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Conference Papers</h2>

<h3><a id="user-content-cvpr" class="anchor" aria-hidden="true" href="#cvpr"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>CVPR</h3>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Krafka-etal2016] Eye Tracking for Everyone. <a href="https://ieeexplore.ieee.org/document/7780608" rel="nofollow">[PDF]</a>
<p><em>Kyle Krafka and Aditya Khosla and Petr Kellnhofer and Harini Kannan and Suchendra Bhandarkar and Wojciech Matusik and Antonio Torralba</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Zhang-etal2015] Appearance-based gaze estimation in the wild. <a href="https://ieeexplore.ieee.org/document/7299081" rel="nofollow">[PDF]</a>
<p><em>Xucong Zhang and Yusuke Sugano and Mario Fritz and Andreas Bulling</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Sugano-etal2014] Learning-by-Synthesis for Appearance-Based 3D Gaze Estimation. <a href="https://ieeexplore.ieee.org/document/6909631" rel="nofollow">[PDF]</a>
<p><em>Yusuke Sugano and Yasuyuki Matsushita and Yoichi Sato</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Sugano-etal2010] Calibration-free gaze sensing using saliency maps. <a href="https://ieeexplore.ieee.org/document/5539984" rel="nofollow">[PDF]</a>
<p><em>Yusuke Sugano and Yasuyuki Matsushita and Yoichi Sato</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Williams-etal2006] Sparse and Semi-supervised Visual Mapping with the S^3GP. <a href="https://ieeexplore.ieee.org/document/1640764" rel="nofollow">[PDF]</a>
<p><em>O. Williams and A. Blake and R. Cipolla</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Zhu-Ji2005] Eye Gaze Tracking under Natural Head Movements. <a href="https://ieeexplore.ieee.org/document/1467364" rel="nofollow">[PDF]</a>
<p><em>Zhiwei Zhu and Qiang Ji</em></p>



<h3><a id="user-content-iccv" class="anchor" aria-hidden="true" href="#iccv"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>ICCV</h3>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Deng-Zhu2017] Monocular Free-Head 3D Gaze Tracking with Deep Learning and Geometry Constraints. <a href="https://ieeexplore.ieee.org/document/8237603" rel="nofollow">[PDF]</a>
<p><em>Haoping Deng and Wangjiang Zhu</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Wood-etal2015] Rendering of Eyes for Eye-Shape Registration and Gaze Estimation. <a href="https://ieeexplore.ieee.org/document/7410785" rel="nofollow">[PDF]</a>
<p><em>Erroll Wood and Tadas Baltruaitis and Xucong Zhang and Yusuke Sugano and Peter Robinson and Andreas Bullingo</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Lu-etal2011a] Inferring human gaze from appearance via adaptive linear regression. <a href="https://ieeexplore.ieee.org/document/6126237" rel="nofollow">[PDF]</a>
<p><em>Feng Lu and Yusuke Sugano and Takahiro Okabe and Yoichi Sato</em></p>



<h3><a id="user-content-eccv" class="anchor" aria-hidden="true" href="#eccv"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>ECCV</h3>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Fischer-etal2018 OpenAccess] RT-GENE: Real-Time Eye Gaze Estimation in Natural Environments. <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Tobias_Fischer_RT-GENE_Real-Time_Eye_ECCV_2018_paper.html" rel="nofollow">[PDF]</a>
<p><em>Tobias Fischer, Hyung Jin Chang, Yiannis Demiris, Yiannis</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Park-etal2018 ArXiv] Deep Pictorial Gaze Estimation. <a href="https://arxiv.org/abs/1807.10002" rel="nofollow">[PDF]</a>
<p><em>Seonwook Park and Adrian Spurr and Otmar Hilliges</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Cheng-etal2018 OpenAccess] Appearance-Based Gaze Estimation via Evaluation-Guided Asymmetric Regression. <a href="http://openaccess.thecvf.com/content_ECCV_2018/html/Yihua_Cheng_Appearance-Based_Gaze_Estimation_ECCV_2018_paper.html" rel="nofollow">[PDF]</a>
<p><em>Cheng, Yihua and Lu, Feng and Zhang, Xucong</em></p>


<h3><a id="user-content-others" class="anchor" aria-hidden="true" href="#others"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Others</h3>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Zhang-etal2017 CVPRW] It's Written All Over Your Face: Full-Face Appearance-Based Gaze Estimation. <a href="https://ieeexplore.ieee.org/document/8015018" rel="nofollow">[PDF]</a>
<p><em>Xucong Zhang and Yusuke Sugano and Mario Fritz and Andreas Bulling</em></p>

<h5><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>[Mora-Odobez2012 CVPRW] Gaze estimation from multimodal Kinect data. <a href="Gaze estimation from multimodal Kinect data" rel="nofollow">[PDF]</a>
<p><em>Kenneth Alberto Funes Mora and Jean-Marc Odobez</em></p>


<h2><a id="user-content-datasets" class="anchor" aria-hidden="true" href="#datasets"><svg class="octicon octicon-link" viewBox="0 0 16 16" version="1.1" width="16" height="16" aria-hidden="true"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Datasets</h2>

<table class="tg">
  <tr>
    <th class="tg-0lax">Dataset<br></th>
    <th class="tg-baqh">RGB/<br>RGB-D<br></th>
    <th class="tg-baqh">Image type</th>
    <th class="tg-baqh">Annotation type</th>
    <th class="tg-baqh">#Images</th>
    <th class="tg-baqh">Distance</th>
    <th class="tg-baqh">Head pose<br>annot.</th>
    <th class="tg-baqh">Gaze<br>annot.<br></th>
    <th class="tg-baqh">Head pose<br>orient.<br></th>
  </tr>
  <tr>
    <td class="tg-0lax">BIWI</td>
    <td class="tg-baqh">RGB-D</td>
    <td class="tg-baqh">Camera frame</td>
    <td class="tg-baqh">Head pose vector</td>
    <td class="tg-baqh">15.500</td>
    <td class="tg-baqh">100cm</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">N</td>
    <td class="tg-baqh">All</td>
  </tr>
  <tr>
    <td class="tg-0lax">CMU Multi-Pie</td>
    <td class="tg-baqh">RGB</td>
    <td class="tg-baqh">Camera frame</td>
    <td class="tg-baqh">68 Facial landmarks</td>
    <td class="tg-baqh">755.370</td>
    <td class="tg-baqh">300cm</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">N</td>
    <td class="tg-baqh">All</td>
  </tr>
  <tr>
    <td class="tg-0lax">Coffeebreak</td>
    <td class="tg-baqh">RGB</td>
    <td class="tg-baqh">Low res. face image</td>
    <td class="tg-baqh">Head pose vector</td>
    <td class="tg-baqh">18.117</td>
    <td class="tg-baqh">Varying</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">N</td>
    <td class="tg-baqh">All</td>
  </tr>
  <tr>
    <td class="tg-0lax">Columbia</td>
    <td class="tg-baqh">RGB</td>
    <td class="tg-baqh">High res. camera image</td>
    <td class="tg-baqh">Gaze vector</td>
    <td class="tg-baqh">5.880</td>
    <td class="tg-baqh">200cm</td>
    <td class="tg-baqh">5 orient.</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Frontal</td>
  </tr>
  <tr>
    <td class="tg-0lax">Deep Head Pose</td>
    <td class="tg-baqh">RGB-D</td>
    <td class="tg-baqh">Camera frame</td>
    <td class="tg-baqh">Head pose vector</td>
    <td class="tg-baqh">68.000</td>
    <td class="tg-baqh">200-800cm</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">N</td>
    <td class="tg-baqh">All</td>
  </tr>
  <tr>
    <td class="tg-0lax">Eyediap</td>
    <td class="tg-baqh">RGB-D</td>
    <td class="tg-baqh">Face + eye patches</td>
    <td class="tg-baqh">Gaze vector</td>
    <td class="tg-baqh">62.500</td>
    <td class="tg-baqh">80-120cm</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Frontal</td>
  </tr>
  <tr>
    <td class="tg-0lax">Gaze Capture</td>
    <td class="tg-baqh">RGB</td>
    <td class="tg-baqh">Face + eye patches</td>
    <td class="tg-baqh">2D pos on screen</td>
    <td class="tg-baqh">> 2.5M</td>
    <td class="tg-baqh">80-120cm</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Frontal</td>
  </tr>
  <tr>
    <td class="tg-0lax">ICT 3D Head pose</td>
    <td class="tg-baqh">RGB-D</td>
    <td class="tg-baqh">Camera frame</td>
    <td class="tg-baqh">Head pose vector</td>
    <td class="tg-baqh">14.000</td>
    <td class="tg-baqh">100cm</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">N</td>
    <td class="tg-baqh">All</td>
  </tr>
  <tr>
    <td class="tg-0lax">MPII Gaze</td>
    <td class="tg-baqh">RGB</td>
    <td class="tg-baqh">Face + eye patches</td>
    <td class="tg-baqh">Gaze vector</td>
    <td class="tg-baqh">213.659</td>
    <td class="tg-baqh">40-60cm</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Frontal</td>
  </tr>
  <tr>
    <td class="tg-0lax">Rice TabletGaze</td>
    <td class="tg-baqh">RGB</td>
    <td class="tg-baqh">Tablet camera video</td>
    <td class="tg-baqh">2D pos on screen</td>
    <td class="tg-baqh">100.000</td>
    <td class="tg-baqh">30-50cm</td>
    <td class="tg-baqh">N</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Frontal</td>
  </tr>
  <tr>
    <td class="tg-0lax">RT-GENE</td>
    <td class="tg-baqh">RGB-D</td>
    <td class="tg-baqh">Face + eye patches</td>
    <td class="tg-baqh">Gaze vector</td>
    <td class="tg-baqh">122.531</td>
    <td class="tg-baqh">80-280cm</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">All</td>
  </tr>
  <tr>
    <td class="tg-0lax">SynthesEyes</td>
    <td class="tg-baqh">RGB</td>
    <td class="tg-baqh">Synthesized eye patches</td>
    <td class="tg-baqh">Gaze vector</td>
    <td class="tg-baqh">11.382</td>
    <td class="tg-baqh">Varying</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">All</td>
  </tr>
  <tr>
    <td class="tg-0lax">UnityEyes</td>
    <td class="tg-baqh">RGB</td>
    <td class="tg-baqh">Synthesized eye patches</td>
    <td class="tg-baqh">Gaze vector</td>
    <td class="tg-baqh">1M</td>
    <td class="tg-baqh">Varying</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">All</td>
  </tr>
  <tr>
    <td class="tg-0lax">UT Multi-view</td>
    <td class="tg-baqh">RGB</td>
    <td class="tg-baqh">Eye area + eye patches</td>
    <td class="tg-baqh">Gaze vector</td>
    <td class="tg-baqh">1.152.000</td>
    <td class="tg-baqh">60cm</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">All</td>
  </tr>
  <tr>
    <td class="tg-0lax">Vernissage</td>
    <td class="tg-baqh">RGB</td>
    <td class="tg-baqh">(Robot) camera frame</td>
    <td class="tg-baqh">Head pose vector</td>
    <td class="tg-baqh">Unknown</td>
    <td class="tg-baqh">Varying</td>
    <td class="tg-baqh">Y</td>
    <td class="tg-baqh">N</td>
    <td class="tg-baqh">All</td>
  </tr>
</table>



